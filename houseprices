#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sat Apr  1 14:11:49 2017

@author: NNK
"""

import pandas as pd
import numpy as np
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFE
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import os

np.random.seed(10)
#%%
os.chdir("/Users/NNK/Documents/project/kaggle/houseprices")

htrain = pd.read_csv('train.csv')
htest = pd.read_csv('test.csv')

htrain = htrain.drop('Id', axis=1)

###
##   Note: change "YearBuilt", "YearSold", "YearRM"
#       to "Years Since" vars..
##
###
htrain['YrsOld'] = 2017 - htrain['YearBuilt']
htrain['YrsRM'] = 2017 - htrain['YearRemodAdd']
htrain['YrsSS'] = 2017 - htrain['YrSold']

htrain = htrain.drop(['YearBuilt','YearRemodAdd','YrSold'],axis=1)

#%%
# Convert dataframe to all numeric for training models
#
#-------------------------
# Fill NAs with 0 first step
htrain = htrain.fillna(value=0)

# Seperate numerical and non-numerical columns into dataframes
numht = htrain.select_dtypes(include = ['float64','int64'])
nonht = htrain.select_dtypes(exclude = ['float64','int64'])

#Create empty df and pass all columns from non-numeric df 
# converted to categorical
non2 = pd.DataFrame()
for column in nonht:
    non2[column] = pd.Categorical(nonht[column])

#Create another empty df and pass all columns from categorical df
# converted to codes    
nonC = pd.DataFrame()
for column in non2:
    nonC[column] = non2[column].cat.codes
    
#Combine the native numerical and newly converted dataframes
trNum = pd.concat([numht,nonC], axis=1)

# Normalize dataframe
trNum_norm = (trNum - trNum.mean()) / trNum.std()

#%%
#Set up training and test sets

train, test = train_test_split(trNum, test_size = .25)

# Train outcome and predictors 
y = train.SalePrice
X = train.drop('SalePrice', axis=1)

# Test outcome and predictors
yt = test.SalePrice
Xt = test.drop('SalePrice', axis=1)


#%%  
#        %------ Perform Feature Selection ------ %
#------------------------------------------------------------------
#KBest with Chi2 test
kbest = SelectKBest(score_func=chi2, k=10)
chifit = kbest.fit(X,y)
features = chifit.transform(X)

chifit._get_param_names

#%%
#Recursive Feature Elimination
lr = LogisticRegression()
rfe = RFE(lr, 10)
rfefit = rfe.fit(X,y)
#-------------------------------------------------------------------


#%% 
#       =========  #PCA  --- Principal Components ==========

#-------------------------------------------------------------------


i = np.identity(trNum_norm.drop('SalePrice', axis=1).shape[1])

pca = PCA(n_components=5, random_state=10)
pca.fit_transform(trNum_norm.drop('SalePrice', axis=1).values)

coef = pca.transform(i)
pcp = pd.DataFrame(coef, columns = ['PC-1','PC-2','PC-3','PC-4','PC-5'],
                           index = trNum_norm.drop('SalePrice', axis=1).columns)

#%% Visualize Principal components

pcp['max'] = pcp.max(axis=1)
pcp['avg'] = pcp.mean(axis=1)
pcp['sum'] = pcp.sum(axis=1)

pcp = pcp.sort(columns=['max','sum','avg'], ascending=False)

plt.rcParams['figure.figsize']=(10,20)
sb.heatmap(pcp[['max','sum','avg']], annot=True, annot_kws={"size": 12})

#%%
#save top components


top = list(pcp[(pcp['sum'] > pcp['sum'].mean())].index)


#%%
#<> ----  Create new train and test sets based on top principal components
#<><>
#<><><>
#<><><><>
#Set up training and test sets

pcdata = pd.concat([trNum[top[0:25]],trNum['SalePrice']],axis=1)

train, test = train_test_split(pcdata, test_size = .25)

# Train outcome and predictors 
y = train.SalePrice
X = train.drop('SalePrice', axis=1)

# Test outcome and predictors
yt = test.SalePrice
Xt = test.drop('SalePrice', axis=1)





#%%
#               ## ==== Model Training ==== ##

#-----------------------------------------------------------------
#Set model parameters
gbfit = GradientBoostingRegressor(n_estimators=500, loss='ls')

#Fit model
gbfit.fit(X=X, y=y)

accuracy = gbfit.score(Xt, yt)
predict = gbfit.predict(Xt)

#%%

# Show results of GBR with all variables

sb.set_style('darkgrid')
plt.scatter(predict, yt)

print('Gradient Boosting Accuracy %s' % '{0:.2%}'.format(accuracy))
#%%
# Model feature importances ranking
importances = gbfit.feature_importances_
indices = np.argsort(importances)[::-1]

print('Feature Importances')

for f in range(X.shape[1]):
    print("feature %s (%f)" % (list(X)[f], importances[indices[f]]))

#plot
feat_imp = pd.DataFrame({'Feature':list(X),
                         'Gini Importance':importances[indices]})

plt.rcParams['figure.figsize']=(8,12)
sb.set_style('whitegrid')
ax = sb.barplot(x='Gini Importance', y='Feature', data=feat_imp)
ax.set(xlabel='Gini Importance')
plt.show()    


#_________________________________________________________________________________>  
#_________________________________________________________________________________>  
#_________________________________________________________________________________>  
#_________________________________________________________________________________>    
#########
########
#######
######
#%%  
#----------------------------------------------------------------------]\\
    #%%
# -------------- Create Bins for SalesPrice Variable --------------------
# ...
## ... for use with Classification Models


bins = [100000,150000,200000,250000,300000,350000,400000,
        450000,500000,550000,600000,650000,700000]
trNum['PriceRange'] = pd.cut(trNum.SalePrice, bins)

#%%
#Set up training and test sets

pcdata = pd.concat([trNum[top[0:25]],trNum['SalePrice']],axis=1)

train, test = train_test_split(pcdata, test_size = .25)

# Train outcome and predictors 
y = train.SalePrice
X = train.drop('SalePrice', axis=1)

# Test outcome and predictors
yt = test.SalePrice
Xt = test.drop('SalePrice', axis=1)





#%%

#%%

#%%

#####
####
###
##
#__________________________#__________________________#__________________________>
#__________________________#__________________________#__________________________>
#__________________________#__________________________#__________________________>
#__________________________#__________________________#__________________________>


#%%
#____________________________<::::::::::::::::::>______________________________
#
#       Re run model with only features with Gini Importance >= 0.01
#
#----------------------------<::::::::::::::::::>______________________________
np.random.seed(10)

trImp = pd.concat([trNum[feat_imp[feat_imp['Gini Importance']>=0.02]['Feature']], trNum['SalePrice']],axis=1)
train, test = train_test_split(trImp, test_size = .25)

# Train outcome and predictors 
y = train.SalePrice
X = train.drop('SalePrice', axis=1)

# Test outcome and predictors
yt = test.SalePrice
Xt = test.drop('SalePrice', axis=1)

#%%
#Set model parameters
gbfit = GradientBoostingRegressor(n_estimators=1500, max_depth=5, loss='ls')

#Fit model
gbfit.fit(X=X, y=y)

accuracy = gbfit.score(Xt, yt)
predict = gbfit.predict(Xt)


sb.set_style('whitegrid')
plt.scatter(predict, yt)