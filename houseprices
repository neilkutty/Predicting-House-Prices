#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sat Apr  1 14:11:49 2017

@author: NNK
"""

import pandas as pd
import numpy as np
import seaborn as sb
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import os

np.random.seed(10)
#%% 
os.chdir("/Users/NNK/Documents/project/kaggle/houseprices")

htrain = pd.read_csv('train.csv')
htest = pd.read_csv('test.csv')

htrain = htrain.drop('Id', axis=1)

###
##   Note: change "YearBuilt", "YearSold", "YearRM"
#       to "Years Since" vars..
##
###
htrain['YrsOld'] = 2017 - htrain['YearBuilt']
htrain['YrsRM'] = 2017 - htrain['YearRemodAdd']
htrain['YrsSS'] = 2017 - htrain['YrSold']

htrain = htrain.drop(['YearBuilt','YearRemodAdd','YrSold'],axis=1)

htrain['TotalSF'] = htrain.GrLivArea + htrain.TotalBsmtSF


#%%
# ------------------- DATA CLEANING ------------------------- #

#...............................................................
# Convert dataframe to all numeric for training models
#
#-------------------------
#htrain = htrain.dropna(axis=1)
# Fill NAs with 0 first step
htrain = htrain.fillna(value=0)
sd_cols = pd.get_dummies(htrain.SaleCondition, prefix='SaleCon')
bt_cols = pd.get_dummies(htrain.BldgType, prefix='BldgType')
style_cols = pd.get_dummies(htrain.HouseStyle, prefix='HouseStyle')
ms_cols = pd.get_dummies(htrain.Exterior1st, prefix='Exterior1st')
nb_cols = pd.get_dummies(htrain.GarageType, prefix='GarageType')
htrain = pd.concat([htrain,sd_cols, bt_cols, style_cols,
                    ms_cols, nb_cols], axis=1)

# Seperate numerical and non-numerical columns into dataframes
numht = htrain.select_dtypes(include = ['float64','int64'])
nonht = htrain.select_dtypes(exclude = ['float64','int64'])

# -- Get Category Codes --
#Create empty df and pass all columns from non-numeric df 
# converted to categorical
non2 = pd.DataFrame()
for column in nonht:
    non2[column] = pd.Categorical(nonht[column])

#Create another empty df and pass all columns from categorical df
# converted to codes    
nonC = pd.DataFrame()
for column in non2:
    nonC[column] = non2[column].cat.codes
    
#Combine the native numerical and newly converted dataframes
trNum = pd.concat([numht,nonC], axis=1)
temp = numht.drop('SalePrice',axis=1)

trnorm = (temp - temp.mean()) / (temp.max() - temp.min())
#Create normalized dataframe
trNum_norm = pd.concat([trnorm,nonC,numht['SalePrice']], axis=1)

normdf = (trNum - trNum.mean()) / (trNum.max() - trNum.min())

#%% 
#%% 
    #--------------- Exploratory Data Analysis ------------------
#---------------------------------------------------------------------
kw = {'axes.edgecolor': '0', 'text.color': '0', 'ytick.color': '0', 
      'xtick.color': '0','ytick.major.size': 5, 'xtick.major.size': 5,
      'axes.labelcolor': '0'}

sb.set_style("darkgrid", kw)

#%% 
#           ------------ Garbage ----------- <><><><><><><><><><><>
sb.lmplot('OverallQual','SalePrice',data=trNum,hue='TotRmsAbvGrd',fit_reg=False)

htrain.ExterQual.value_counts()

sfcols = [col for col in htrain.columns if 'SF' in col]
htrain[sfcols].head
#...........
#%%

sb.lmplot('GrLivArea','SalePrice',trNum, hue='TotRmsAbvGrd', fit_reg=False)

#%%

#%%

#%%

#............
#.............
#..............
#...............

#%%
#     >>> Correlation Plots

plt.rcParams['figure.figsize']=(8,14)
corrdf = trNum.corr()
ycorr = corrdf[['SalePrice']]
ycorr = ycorr.sort_values(by=['SalePrice'])
ycorr.drop(['SalePrice']).plot(kind='barh')


#%%  

#       =========  #Plotly - Exploratory  ==========
import plotly
import plotly.plotly as py
import plotly.graph_objs as go


plotly.tools.set_credentials_file(username='sampsonsimpson', 
                                  api_key='cxm2iF7KKBGZgXmDOU9S')

layout = go.Layout(
        title='House Price vs. Living Area'
        )

d = [
     go.Scatter(
             x=trNum['GrLivArea'],
             y=trNum['SalePrice'],
             mode = "markers",
             marker= dict(size= 14,
                    line= dict(width=1),
                    color= trNum['SalePrice'],
                    opacity= 0.7
                   ))]


fig = go.Figure(data=d, layout=layout)

py.plot(fig, filename='house-prices')



#%% 





#       =========  #PCA  --- Principal Components ==========

#-------------------------------------------------------------------

#Create a different normalized dataframe for PCA
trPCA = (trNum - trNum.mean()) / trNum.std()

i = np.identity(trPCA.drop('SalePrice', axis=1).shape[1])

pca = PCA(n_components=5, random_state=1010)
pca.fit_transform(trPCA.drop('SalePrice', axis=1).values)

coef = pca.transform(i)
pcp = pd.DataFrame(coef, columns = ['PC-1','PC-2','PC-3','PC-4','PC-5'],
                           index = trPCA.drop('SalePrice', axis=1).columns)

#%% Visualize Principal components

pcp['max'] = pcp.max(axis=1)
pcp['avg'] = pcp.mean(axis=1)
pcp['sum'] = pcp.sum(axis=1)

pcp = pcp.sort_values(by=['max'], ascending=False)

plt.rcParams['figure.figsize']=(10,20)
sb.heatmap(pcp[['max','sum','avg']], annot=True, annot_kws={"size": 12})

#%% Combine PCA results and Pearson Correlation results ---<><><><><><><><><><>

# --------------------- <><><><>< * * * * * * * * * * * * * * * * * ><><><><>

ycorr = ycorr.rename(columns={'SalePrice':'YCorrelation'})
fsel = pd.concat([ycorr.drop('SalePrice',axis=0), pcp], axis=1)

#%%
# -----  -----  save top components

#conditional top
top = list(pcp[(pcp['sum'] > pcp['sum'].mean())].index)

#sorted list
pclist = list(pcp.index)

#%%
#<> ----  Create new train and test sets based on top principal components ---
#<><>
#<><><>
#<><><><>
#Set up training and test sets

pcdata = pd.concat([trNum[pclist[0:len(pclist)]],trNum['SalePrice']],axis=1)
nmdata = pd.concat([trNum_norm[pclist[0:len(pclist)]],trNum_norm['SalePrice']],axis=1)
#nmonly = pd.concat([trNum_norm[list(numht.drop('SalePrice',axis=1))],trNum_norm['SalePrice']],axis=1)
nmonly = normdf
train, test = train_test_split(pcdata, test_size = .30, random_state = 1010)

# Train outcome and predictors 
y = train.SalePrice
X = train.drop('SalePrice', axis=1)

# Test outcome and predictors
yt = test.SalePrice
Xt = test.drop('SalePrice', axis=1)

# Create normalized train and test sets

train, test = train_test_split(nmonly, test_size = .30, random_state = 1010)

ynorm = train.SalePrice
Xnorm = train.drop('SalePrice', axis=1)

ytnorm = test.SalePrice
Xtnorm = test.drop('SalePrice', axis=1)




#%%
#               ## ==== Model Training ==== ##
#
#               ## ==== Gradient Boosting Regressor ==== ##

#-----------------------------------------------------------------
#Set model parameters
gbfit = GradientBoostingRegressor(n_estimators=250, loss='ls', random_state=1010)

#Fit model
gbfit.fit(X=X, y=y)

#%% explore GB fit
accuracy = gbfit.score(Xt, yt)
predict = gbfit.predict(Xt)

#%% GB with Normalized variables
gbfit.fit(X=Xnorm, y=ynorm)
accuracy = gbfit.score(Xtnorm,ytnorm)
predict = gbfit.predict(Xtnorm)


#%%
# Show results of GBR with all variables

sb.set_style('darkgrid')
plt.rcParams['figure.figsize']=(10,8)
plt.scatter(predict, yt)
plt.suptitle('test title')
plt.xlabel('Predicted')
plt.ylabel('Ground Truth')

print('Gradient Boosting Accuracy %s' % '{0:.2%}'.format(accuracy))
#%%
# Model feature importances ranking
importances = gbfit.feature_importances_
indices = np.argsort(importances)[::-1]

print('Feature Importances')

for f in range(X.shape[1]):
    print("feature %s (%f)" % (list(X)[f], importances[indices[f]]))

#plot
feat_imp = pd.DataFrame({'Feature':list(X),
                         'Gini Importance':importances[indices]})

plt.rcParams['figure.figsize']=(8,12)
sb.set_style('whitegrid')
ax = sb.barplot(x='Gini Importance', y='Feature', data=feat_imp)
ax.set(xlabel='Gini Importance')
plt.show()    

#_________________________________________________________________________________>  
#_________________________________________________________________________________> 

# End Gradient Boosting .. 
#_________________________________________________________________________________>  
#_________________________________________________________________________________>  
#_________________________________________________________________________________>    
#########

#%%#   ------- Linear Models --------
reg = linear_model.SGDRegressor(alpha=1)
reg.fit(Xnorm,ynorm)
accuracy = reg.score(Xtnorm,ytnorm)
########
#######
#%%
#%%
#%%
#
# --------------------- MLP Regressor ---------------------------
#
######

mlpreg = MLPRegressor(solver='lbfgs', alpha=50, hidden_layer_sizes=(500,), random_state=10)
mlpRfit = mlpreg.fit(Xnorm, ynorm)
accuracy = mlpRfit.score(Xtnorm, ytnorm)


#%%
#%%  
#%%  
#%%  
#%%  
#%%  



#----------------------------------------------------------------------------------------]\\
#----------------------------------------------------------------------------------------]\\
#----------------------------------------------------------------------------------------]\\
#----------------------------------------------------------------------------------------]\\
    #%%
# -------------- Create Bins for SalesPrice Variable --------------------
# ...
## ... for use with Classification Models


#bins = [0,100000,150000,200000,250000,300000,350000,400000,
##       450000,500000,550000,600000,650000,700000]

# .. need to create labels vector for 'mix type error (string and num)

bins=7
trNum['PriceRange'] = pd.cut(trNum.SalePrice, bins)
trNum_norm['PriceRange'] = pd.cut(trNum.SalePrice, bins)

#%%
#Set up training and test sets for classification

cats = pd.concat([trNum_norm[pclist[0:len(pclist)]],trNum_norm['PriceRange']],axis=1)

train, test = train_test_split(cats, test_size = .25, random_state=10)

# Train outcome and predictors 
y = train.PriceRange
X = train.drop('PriceRange', axis=1)

# Test outcome and predictors
yt = test.PriceRange
Xt = test.drop('PriceRange', axis=1)



#%%
# >>>>>>>>>>>>>>> Random Forest <<<<<<<<<<<<<<<<<<<<<

forest = RandomForestClassifier(n_estimators=1000, random_state=1010)
rfit = forest.fit(X,y)
accuracy = rfit.score(Xt,yt)



#%%
# >>>>>>>>>>>>>>> MLP Classifier <<<<<<<<<<<<<<<<<<<<<

mlp = MLPClassifier(solver='lbfgs', alpha=5, hidden_layer_sizes=(500,), random_state=10)

fit = mlp.fit(X,y)
accuracy = fit.score(Xt,yt)

#%%
